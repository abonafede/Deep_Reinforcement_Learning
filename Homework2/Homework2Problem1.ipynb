{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPqwyQ1CCI6dvZNaZSEu0Ok"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"pzo-zEmtq1OB","executionInfo":{"status":"ok","timestamp":1677081957641,"user_tz":300,"elapsed":3,"user":{"displayName":"andrew B","userId":"00947509349477408009"}}},"outputs":[],"source":["NAME = \"ANDREW BONAFEDE\""]},{"cell_type":"markdown","source":["# Problem 1: Implement Double DQN"],"metadata":{"id":"3FSxtD57u3Hq"}},{"cell_type":"code","source":["# Install my version of stable-baselines3 from Github\n","!pip install git+\"https://github.com/abonafede/stable-baselines3\""],"metadata":{"id":"zcJ5277Au_01"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import Necessary Libraries\n","import numpy as np\n","import pandas as pd\n","import gym\n","from stable_baselines3 import DQN\n","from stable_baselines3.common.evaluation import evaluate_policy"],"metadata":{"id":"ueJ6o18rvHlD","executionInfo":{"status":"ok","timestamp":1677081968381,"user_tz":300,"elapsed":2108,"user":{"displayName":"andrew B","userId":"00947509349477408009"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Vanilla DQN (Default)\n","Utilizing our favorite Cartpoole-v1"],"metadata":{"id":"qZmpy7UAvfCu"}},{"cell_type":"code","source":["# Create the Necessary Gym Environments\n","vanilla_train_env = gym.make('CartPole-v1')\n","vanilla_eval_env = gym.make('CartPole-v1')\n","\n","# Create our Vanilla DQN\n","dqn_vanilla = DQN('MlpPolicy', vanilla_train_env, verbose=1,double_dqn=False)"],"metadata":{"id":"iyaufKhHvmdE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate before Training\n","reward_mean, reward_std = evaluate_policy(dqn_vanilla, vanilla_eval_env, n_eval_episodes=100)\n","print(f'DQN Mean Reward: {np.round(reward_mean,2)} +/- {np.round(reward_std,2)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZOHdU_vtwR3_","executionInfo":{"status":"ok","timestamp":1677081970204,"user_tz":300,"elapsed":1552,"user":{"displayName":"andrew B","userId":"00947509349477408009"}},"outputId":"f1cb0714-3374-4a3b-fb4e-04df0c334092"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["DQN Mean Reward: 24.29 +/- 30.33\n"]}]},{"cell_type":"code","source":["# Train the Vanilla DQN\n","dqn_vanilla.learn(total_timesteps=200000)"],"metadata":{"id":"5yWMAHaLxDCm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Evaluate the Trained Vanilla DQN\n","reward_mean, reward_std = evaluate_policy(dqn_vanilla, vanilla_eval_env, n_eval_episodes=100)\n","print(f'Mean Reward: {np.round(reward_mean,2)} +/- {np.round(reward_std,2)}')"],"metadata":{"id":"enzmInlbxaW8","executionInfo":{"status":"ok","timestamp":1677082139055,"user_tz":300,"elapsed":7647,"user":{"displayName":"andrew B","userId":"00947509349477408009"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d01d495d-5dfa-4f27-b5e0-5ef7fbd88f93"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Reward: 220.1 +/- 46.27\n"]}]},{"cell_type":"markdown","source":["# Double DQN\n","Utilizing our favorite Cartpole-v1"],"metadata":{"id":"3qLSAigC2phf"}},{"cell_type":"code","source":["# Create the Necessary Gym Environments\n","double_dqn_train_env = gym.make('CartPole-v1')\n","double_dqn_eval_env = gym.make('CartPole-v1')\n","\n","# Create our Double DQN\n","double_dqn = DQN('MlpPolicy', double_dqn_train_env, verbose=1,double_dqn=True)"],"metadata":{"id":"3lhvR64721RN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate before Training\n","reward_mean, reward_std = evaluate_policy(double_dqn, double_dqn_eval_env, n_eval_episodes=100)\n","print(f'DQN Mean Reward: {np.round(reward_mean,2)} +/- {np.round(reward_std,2)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kDEn_l-W3DzJ","executionInfo":{"status":"ok","timestamp":1677082139338,"user_tz":300,"elapsed":286,"user":{"displayName":"andrew B","userId":"00947509349477408009"}},"outputId":"d3522dc6-902d-4be4-f11d-4fc16c915984"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["DQN Mean Reward: 11.11 +/- 3.12\n"]}]},{"cell_type":"code","source":["# Train the Double DQN\n","double_dqn.learn(total_timesteps=200000)"],"metadata":{"id":"-70X-YKT3JlW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Evaluate the Trained Double DQN\n","reward_mean, reward_std = evaluate_policy(double_dqn, double_dqn_eval_env, n_eval_episodes=100)\n","print(f'Mean Reward: {np.round(reward_mean,2)} +/- {np.round(reward_std,2)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y9JjE-Rc3jmv","executionInfo":{"status":"ok","timestamp":1677082311987,"user_tz":300,"elapsed":10167,"user":{"displayName":"andrew B","userId":"00947509349477408009"}},"outputId":"0ff50e99-537b-4fa0-df35-8803b4f018dd"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Reward: 270.75 +/- 64.43\n"]}]}]}