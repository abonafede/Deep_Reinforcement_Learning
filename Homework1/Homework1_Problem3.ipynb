{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM4Azre83O5fPvYxoeAbr10"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"ZUq26Ekd76IT","executionInfo":{"status":"ok","timestamp":1675892025013,"user_tz":300,"elapsed":10,"user":{"displayName":"andrew B","userId":"00947509349477408009"}}},"outputs":[],"source":["NAME= 'Andrew Bonafede'"]},{"cell_type":"markdown","source":["# Problem 3\n","\n","Compare the n-step advantage with n-step return (mentioned in\n","the class), vanilla advantage, GAE, as well as MC advantage for\n","A2C algorithm"],"metadata":{"id":"m4Z__cFM8FB8"}},{"cell_type":"markdown","source":["# My Solution:\n","\n","Inputting gae_lambda: 1 ⇒ Default GAE \\\\\n","Inputting gae_lambda: 0 ⇒ 1-Step (Vanilla) Advantage \\\\\n","Inputting gae_lambda: -1 ⇒ N-Step Return \\\\\n","# We will be testing this on the Cartpole-v1 for each."],"metadata":{"id":"C6NswD_N8Qfj"}},{"cell_type":"code","source":["# Install my version of stable-baselines3 from Github\n","!pip install git+\"https://github.com/abonafede/stable-baselines3\""],"metadata":{"id":"HHDrJfYS8n0c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import Necessary Libraries\n","# Import necessary libraries\n","import numpy as np\n","import pandas as pd\n","import gym\n","from stable_baselines3 import A2C\n","from stable_baselines3.common.evaluation import evaluate_policy"],"metadata":{"id":"_sBr-KXY9nLj","executionInfo":{"status":"ok","timestamp":1675892280625,"user_tz":300,"elapsed":4453,"user":{"displayName":"andrew B","userId":"00947509349477408009"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# N-Step Return\n","Set gae_lambda = -1"],"metadata":{"id":"fcpsNXFp9Zc8"}},{"cell_type":"code","source":["# Create the Necessary Gym Environments\n","# Create our Agent Instance\n","train_env = gym.make('CartPole-v1')\n","eval_env = gym.make('CartPole-v1')\n","\n","#Create our A2C Agent\n","model = A2C('MlpPolicy', train_env, verbose=1,gae_lambda=-1)\n","model.learn(total_timesteps=100000)"],"metadata":{"id":"M_Xp7Bui9eO2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate the agent\n","reward_mean, reward_std = evaluate_policy(model, eval_env, n_eval_episodes=100)\n","print(f'Mean Reward: {np.round(reward_mean,2)} +/- {np.round(reward_std,2)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aqNbVltp_sXs","executionInfo":{"status":"ok","timestamp":1675892878682,"user_tz":300,"elapsed":31475,"user":{"displayName":"andrew B","userId":"00947509349477408009"}},"outputId":"a4706a61-5332-4749-fa7f-e636c73b3eac"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Mean Reward: 500.0 +/- 0.0\n"]}]},{"cell_type":"markdown","source":["# 1-Step (Vanilla) Advantage\n","Set gae_lambda = 0"],"metadata":{"id":"ume8OUJBAB42"}},{"cell_type":"code","source":["# Create the Necessary Gym Environments\n","# Create our Agent Instance\n","train_env = gym.make('CartPole-v1')\n","eval_env = gym.make('CartPole-v1')\n","\n","#Create our A2C Agent\n","model = A2C('MlpPolicy', train_env, verbose=1,gae_lambda=0)\n","model.learn(total_timesteps=100000)"],"metadata":{"id":"ViPjMgcdAHwP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluating the agent using 100 episodes\n","reward_mean, reward_std = evaluate_policy(model, eval_env, n_eval_episodes=100)\n","print(f'Mean Reward: {np.round(reward_mean,2)} +/- {np.round(reward_std,2)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JoImW7PyBAwR","executionInfo":{"status":"ok","timestamp":1675893154269,"user_tz":300,"elapsed":1651,"user":{"displayName":"andrew B","userId":"00947509349477408009"}},"outputId":"e960af58-fe53-4e4d-e61d-c11473e29a7a"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Reward: 26.33 +/- 3.69\n"]}]},{"cell_type":"markdown","source":["# Default GAE\n","Set gae_lambda = 1"],"metadata":{"id":"NAss-DX2BHKK"}},{"cell_type":"code","source":["# Create the Necessary Gym Environments\n","# Create our Agent Instance\n","train_env = gym.make('CartPole-v1')\n","eval_env = gym.make('CartPole-v1')\n","\n","#Create our A2C Agent\n","model = A2C('MlpPolicy', train_env, verbose=1,gae_lambda=1)\n","model.learn(total_timesteps=100000)"],"metadata":{"id":"5FWwKJTfBMzt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluating the agent using 100 episodes\n","reward_mean, reward_std = evaluate_policy(model, eval_env, n_eval_episodes=100)\n","print(f'Mean Reward: {np.round(reward_mean,2)} +/- {np.round(reward_std,2)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yp2MmkmxB6Xo","executionInfo":{"status":"ok","timestamp":1675893418727,"user_tz":300,"elapsed":31369,"user":{"displayName":"andrew B","userId":"00947509349477408009"}},"outputId":"45bc78f8-a11d-44a2-ab77-4d6b6e64e528"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Reward: 500.0 +/- 0.0\n"]}]}]}